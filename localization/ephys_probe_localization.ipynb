{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8124e52d-efa7-47b6-9e74-d316754f03e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import gridspec\n",
    "import seaborn as sns\n",
    "import os, re\n",
    "import glob\n",
    "import _pickle as pkl\n",
    "import scipy.signal as signal\n",
    "import math\n",
    "import pandas as pd\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2Â \n",
    "sns.set_style(\"white\")\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from dlab import generalephys as ephys\n",
    "import dlab as traces\n",
    "import dlab as utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2bd7a90e-27b0-45f4-bc35-32bbd521a4c0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def count_all_peaks(file_path, sigma_thresh=5):\n",
    "    bad_channels = []\n",
    "    traces.skip_channels = np.append(traces.npix_p3_reference_channels, bad_channels)\n",
    "    \n",
    "    mm = np.memmap(file_path, dtype=np.int16, mode='r')\n",
    "    \n",
    "    num_channels = traces.get_channel_count(file_path.rsplit(os.path.sep, 1)[0], from_channel_map=False)\n",
    "    chunk = traces.get_chunk(mm, 3., 11., num_channels, sampling_rate = 30000)\n",
    "    \n",
    "    norm_chunk, mean_offset = get_norm_chunk(chunk)\n",
    "    \n",
    "    peaks = np.zeros((norm_chunk.shape[0] - len(traces.skip_channels), 1))\n",
    "    dead_channels_passed = 0\n",
    "    \n",
    "    for channel in range(norm_chunk.shape[0]):\n",
    "        if channel not in traces.skip_channels:\n",
    "            peaks[channel - dead_channels_passed], _, _ = count_channel_peaks(norm_chunk[channel,:], sigma_thresh)\n",
    "        else:\n",
    "            dead_channels_passed += 1\n",
    "        \n",
    "    return peaks, mean_offset\n",
    "\n",
    "def count_channel_peaks(signal, sigma_thresh=5):\n",
    "    \n",
    "    num_peaks = 0\n",
    "    indices = []\n",
    "    peak_values = []\n",
    "    threshold = sigma_thresh * np.std(signal) + np.mean(signal)\n",
    "    \n",
    "    above_thresh = False\n",
    "    max_point = 0\n",
    "    \n",
    "    for index, point in enumerate(signal):\n",
    "        if above_thresh:\n",
    "            if point < threshold * 0.5:\n",
    "                indices.append(peak_index)\n",
    "                peak_values.append(max_point)\n",
    "                max_point = 0\n",
    "                above_thresh = False\n",
    "            else: \n",
    "                if point > max_point:\n",
    "                    max_point = point\n",
    "                    peak_index = index\n",
    "                \n",
    "        else:\n",
    "            if point >= threshold:\n",
    "                above_thresh = True\n",
    "                num_peaks = num_peaks + 1\n",
    "                peak_index = index\n",
    "                max_point = point\n",
    "                \n",
    "    return num_peaks, indices, peak_values\n",
    "\n",
    "def get_norm_chunk(chunk):\n",
    "    chunk_detrended = np.zeros(chunk.shape)\n",
    "    mean_offset = np.zeros((chunk.shape[0], 1))\n",
    "    \n",
    "    for channel in range(chunk.shape[0]):\n",
    "        chunk_detrended[channel, :] = butter_highpass_filter(chunk[channel,:], 10, 30000, order=5)\n",
    "        mean_offset[channel, :] = np.mean(chunk[channel,:])\n",
    "    \n",
    "    return chunk_detrended, mean_offset\n",
    "\n",
    "def butter_highpass(cutoff, fs, order=5):\n",
    "    nyq = 0.5 * fs\n",
    "    normal_cutoff = cutoff / nyq\n",
    "    b, a = signal.bessel(order, normal_cutoff, btype='highpass', analog=False)\n",
    "    w, h = signal.freqs(b, a)\n",
    "    return b, a\n",
    "\n",
    "def butter_highpass_filter(data, cutoff, fs, order=5):\n",
    "    b, a = butter_highpass(cutoff, fs, order=order)\n",
    "    y = signal.filtfilt(b, a, data)\n",
    "    return y\n",
    "\n",
    "def running_mean(x, N):\n",
    "    return np.expand_dims(np.convolve(x[:,0], np.ones((N,))/N)[(N-1) / 2:-(N-1) / 2], 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6de5eaf-be53-45c2-aa8f-7010f7e90bac",
   "metadata": {},
   "source": [
    "## Choose Files for Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9b83d81b-d5a9-465b-9458-24c0c83c37ec",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "path = os.path.join(r'\\Users\\danieljdenman','Desktop')\n",
    "\n",
    "# filenames_spikeband = glob.glob(os.path.join(path,'*','*0_0.dat'))\n",
    "# filenames_lfp = glob.glob(os.path.join(path,'*','*1_0.dat'))\n",
    "\n",
    "# phase = 3\n",
    "# bad_channels = []\n",
    "# traces.skip_channels = np.append(traces.npix_p3_reference_channels, bad_channels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "941b88d2-b59f-434e-af68-6619f34a4cb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  36.   75.  112.  151.  188.  227.  264.  303.  340.  379.]\n"
     ]
    }
   ],
   "source": [
    "print traces.skip_channels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df03b9af-d6ba-40de-a3b1-30fc1d21d385",
   "metadata": {},
   "source": [
    "# Download this dataset to your local machine start:\n",
    "localization/M310016_2017-06-15_09-11-15_3\n",
    "### this can be done through denmanlab.quickconnect.to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae45f9e6-0a09-4a85-b340-fe3f017bde7e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f51c6b9e-ae58-409f-b4bf-0993da46a9df",
   "metadata": {},
   "source": [
    "## Get Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "154c391c-4ee9-4cf8-abed-e8e6f36a7087",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys([b'/Volumes/SD1/DanD/M310016/localization/M310016_2017-06-15_09-11-15_3', b'/Volumes/SD1/DanD/M310016/localization/M310016_2017-06-15_08-43-17_2', b'/Volumes/SD1/DanD/M310016/localization/M310016_2017-06-15_09-55-55_4b', b'/Volumes/SD1/DanD/M310016/localization/M310016_2017-06-15_11-21-18_7', b'/Volumes/SD1/DanD/M310016/localization/M310016_2017-06-15_11-51-37_8', b'/Volumes/SD1/DanD/M310016/localization/M310016_2017-06-15_09-44-15_4', b'/Volumes/SD1/DanD/M310016/localization/M310016_2017-06-15_10-25-04_5', b'/Volumes/SD1/DanD/M310016/localization/M310016_2017-06-15_08-10-38_1', '/Volumes/SD1/DanD/M270512/270512_2016-12-16_10-51-44'])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "047e8ba7-82e8-4ef7-920c-808665bc6a90",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_path = 'labels.pkl'\n",
    "labels = pkl.load(open(labels_path,'rb'))\n",
    "\n",
    "## This just takes the first dictionary keys and shortens them down a little to just something like \n",
    "## M310016_2017-06-15_08-10-38_1 instead of the whole filename\n",
    "# for key in labels.keys():\n",
    "#     print(type(str(key)))#.split('/')\n",
    "#     labels[str(key).split('/')[-1]]# = labels.pop(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8e4cc198-54b5-4686-8fe0-6d17685674dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{b'/Volumes/SD1/DanD/M310016/localization/M310016_2017-06-15_10-52-16_6': {'ventral thalamus': (0,\n",
       "   39),\n",
       "  'primary somatosensory cortex': (279, 351),\n",
       "  'white matter': (40, 47),\n",
       "  'above': (352, 383),\n",
       "  'lateral geniculate': (48, 88),\n",
       "  'CA3': (89, 148),\n",
       "  'CA1': (190, 268),\n",
       "  'dentate gyrus': (149, 189)},\n",
       " b'/Volumes/SD1/DanD/M310016/localization/M310016_2017-06-15_09-11-15_3': {'secondary motor cortex': (184,\n",
       "   331),\n",
       "  'lateral ventricle': (44, 160),\n",
       "  'white matter': (161, 183),\n",
       "  'above': (332, 383),\n",
       "  'fimbria': (0, 43)},\n",
       " b'/Volumes/SD1/DanD/M310016/localization/M310016_2017-06-15_08-43-17_2': {'caudate putamen': (72,\n",
       "   185),\n",
       "  'primary motor cortex': (215, 345),\n",
       "  'white matter': (186, 214),\n",
       "  'above': (346, 383),\n",
       "  'globus pallidus': (0, 71)},\n",
       " b'/Volumes/SD1/DanD/M310016/localization/M310016_2017-06-15_09-55-55_4b': {'primary somatosensory cortex': (268,\n",
       "   339),\n",
       "  'lateral ventricle': (147, 243),\n",
       "  'white matter': (244, 267),\n",
       "  'above': (340, 383),\n",
       "  'VPM': (0, 37),\n",
       "  'LDVL': (69, 104),\n",
       "  'fimbria': (105, 146),\n",
       "  'Po': (38, 68)},\n",
       " b'/Volumes/SD1/DanD/M310016/localization/M310016_2017-06-15_11-21-18_7': {'lateral posterior': (80,\n",
       "   103),\n",
       "  'white matter': (269, 278),\n",
       "  'above': (344, 383),\n",
       "  'ventral thalamus': (0, 33),\n",
       "  'CA1': (179, 268),\n",
       "  'dentate gyrus': (104, 178),\n",
       "  'parietal cortex': (279, 343),\n",
       "  'Po': (34, 79)},\n",
       " b'/Volumes/SD1/DanD/M310016/localization/M310016_2017-06-15_11-51-37_8': {'caudate putamen': (0,\n",
       "   191),\n",
       "  'primary somatosensory cortex': (220, 353),\n",
       "  'white matter': (192, 219),\n",
       "  'above': (354, 383)},\n",
       " b'/Volumes/SD1/DanD/M310016/localization/M310016_2017-06-15_09-44-15_4': {'primary somatosensory cortex': (268,\n",
       "   339),\n",
       "  'lateral ventricle': (147, 243),\n",
       "  'white matter': (244, 267),\n",
       "  'above': (340, 383),\n",
       "  'VPM': (0, 37),\n",
       "  'LDVL': (69, 104),\n",
       "  'fimbria': (105, 146),\n",
       "  'Po': (38, 68)},\n",
       " b'/Volumes/SD1/DanD/M310016/localization/M310016_2017-06-15_10-25-04_5': {'ventral thalamus': (23,\n",
       "   131),\n",
       "  'lateral ventricle': (242, 262),\n",
       "  'white matter': (263, 274),\n",
       "  'above': (352, 383),\n",
       "  'lateral dorsal': (132, 179),\n",
       "  'zona incerta': (0, 22),\n",
       "  'primary motor cortex': (275, 351),\n",
       "  'fimbria': (180, 241)},\n",
       " b'/Volumes/SD1/DanD/M310016/localization/M310016_2017-06-15_08-10-38_1': {'nucleus accumbens': (0,\n",
       "   20),\n",
       "  'caudate putamen': (21, 171),\n",
       "  'primary motor cortex': (202, 350),\n",
       "  'white matter': (172, 201),\n",
       "  'above': (351, 383)},\n",
       " '/Volumes/SD1/DanD/M270512/270512_2016-12-16_10-51-44': {'above': (318, 383),\n",
       "  'primary visual cortex': (191, 353),\n",
       "  'subiculum': (143, 174),\n",
       "  'white matter': (175, 190),\n",
       "  'dentate gyrus': (59, 142),\n",
       "  'medial geniculate': (0, 58)}}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "449ad681-1b39-407a-baf3-519c62bd5e49",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'filenames_lfp' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-32-750cb9db3b49>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m## Display all the annotations for a quick reference when I need to look at something\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdirname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilenames_lfp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mdirkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdirname\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mstructure\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdirkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'filenames_lfp' is not defined"
     ]
    }
   ],
   "source": [
    "## Display all the annotations for a quick reference when I need to look at something\n",
    "for i, dirname in enumerate(filenames_lfp):\n",
    "    dirkey = dirname.split(os.path.sep)[-2]\n",
    "    print(dirkey)\n",
    "    for structure in labels[dirkey].iterkeys():\n",
    "        print('             ', structure, labels[dirkey][structure][0], labels[dirkey][structure][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "014d6b75-f3b2-4e97-9fbf-4e6de1375eea",
   "metadata": {},
   "source": [
    "## GETTING DATA (NOT CHUNKED)\n",
    "\n",
    "Meaning for each channel individually. Chunking it comes later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "926c8044-0b6e-4ee0-8b41-1fbf4d8e50de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M310016_2017-06-15_08-10-38_1 0\n",
      "M310016_2017-06-15_08-43-17_2 1\n",
      "M310016_2017-06-15_09-11-15_3 2\n",
      "M310016_2017-06-15_09-44-15_4 3\n",
      "M310016_2017-06-15_09-55-55_4b 4\n",
      "M310016_2017-06-15_10-25-04_5 5\n",
      "M310016_2017-06-15_10-52-16_6 6\n",
      "M310016_2017-06-15_11-21-18_7 7\n",
      "M310016_2017-06-15_11-51-37_8 8\n"
     ]
    }
   ],
   "source": [
    "target_all_ = np.empty((0,1))\n",
    "rms_spike_all_ = np.empty((0,1))\n",
    "rms_lfp_all_ = np.empty((0,1))\n",
    "gamma_all_ = np.empty((0,1))\n",
    "alpha_all_ = np.empty((0,1))\n",
    "beta_all_ = np.empty((0,1))\n",
    "delta_all_ = np.empty((0,1))\n",
    "theta_all_ = np.empty((0,1))\n",
    "peaks_all_ = np.empty((0,1))\n",
    "mean_offset_all_ = np.empty((0,1))\n",
    "row_all_ = np.empty((0,1))\n",
    "\n",
    "for i in range(len(filenames_lfp)):\n",
    "    spike_file = filenames_spikeband[i]\n",
    "    lfp_file = filenames_lfp[i]\n",
    "    dirkey = spike_file.split(os.path.sep)[-2]\n",
    "    print dirkey, i\n",
    "    \n",
    "    rms_spike_ = np.expand_dims(traces.get_probe_freq(spike_file, frequency_range=[300, 30000]), 1)\n",
    "    rms_lfp_ = np.expand_dims(traces.get_probe_freq(lfp_file, frequency_range=[0, 300]), 1)\n",
    "    gamma_ = np.expand_dims(traces.get_probe_freq(lfp_file, frequency_range=[30, 40]), 1)\n",
    "    alpha_ = np.expand_dims(traces.get_probe_freq(lfp_file, frequency_range=[8, 12]), 1)\n",
    "    beta_ = np.expand_dims(traces.get_probe_freq(lfp_file, frequency_range=[12, 30]), 1)\n",
    "    delta_ = np.expand_dims(traces.get_probe_freq(lfp_file, frequency_range=[0, 4]), 1)\n",
    "    theta_ = np.expand_dims(traces.get_probe_freq(lfp_file, frequency_range=[4, 8]), 1)\n",
    "    peaks_, mean_offset_ = count_all_peaks(spike_file, 4)\n",
    "    peaks_ = running_mean(peaks_, 10)\n",
    "    row_ = np.expand_dims([pix // 2 for pix in range(384)], 1)\n",
    "    \n",
    "    target_ = [\"\" for j in range(384)]\n",
    "    \n",
    "    for structure in labels[dirkey].iterkeys():\n",
    "        for index in np.arange(labels[dirkey][structure][0], labels[dirkey][structure][1] + 1):\n",
    "            target_[index] = structure\n",
    "    \n",
    "    for skip in sorted(traces.skip_channels, reverse=True):\n",
    "        target_.pop(int(skip))\n",
    "        row_ = np.delete(row_, int(skip), 0)\n",
    "        mean_offset_ = np.delete(mean_offset_, int(skip), 0)\n",
    "    \n",
    "    ## 82nd channel is always weird and can't add it to bad channels because then a different channel becomes weird, and so on\n",
    "    rms_spike_[82,:] = (rms_spike_[81,:] + rms_spike_[83,:]) / 2.0\n",
    "    rms_lfp_[82,:] = (rms_lfp_[81,:] + rms_lfp_[83,:]) / 2.0\n",
    "    gamma_[82,:] = (gamma_[81,:] + gamma_[83,:]) / 2.0\n",
    "    alpha_[82,:] = (alpha_[81,:] + alpha_[83,:]) / 2.0\n",
    "    beta_[82,:] = (beta_[81,:] + beta_[83,:]) / 2.0\n",
    "    delta_[82,:] = (delta_[81,:] + delta_[83,:]) / 2.0\n",
    "    theta_[82,:] = (theta_[81,:] + theta_[83,:]) / 2.0\n",
    "    \n",
    "    rms_spike_all_ = np.append(rms_spike_all_, rms_spike_, axis=0)\n",
    "    rms_lfp_all_ = np.append(rms_lfp_all_, rms_lfp_, axis=0)\n",
    "    gamma_all_ = np.append(gamma_all_, gamma_, axis=0)\n",
    "    alpha_all_ = np.append(alpha_all_, alpha_, axis=0)\n",
    "    beta_all_ = np.append(beta_all_, beta_, axis=0)\n",
    "    delta_all_ = np.append(delta_all_, delta_, axis=0)\n",
    "    theta_all_ = np.append(theta_all_, theta_, axis=0)\n",
    "    peaks_all_ = np.append(peaks_all_, peaks_, axis=0)\n",
    "    mean_offset_all_ = np.append(mean_offset_all_, mean_offset_, axis=0)\n",
    "    row_all_ = np.append(row_all_, row_, axis=0)\n",
    "    target_all_ = np.append(target_all_, np.expand_dims(target_, 1), axis=0)\n",
    "\n",
    "all_bands_data = np.concatenate((rms_spike_all_, rms_lfp_all_, gamma_all_, alpha_all_, \n",
    "                                 beta_all_, delta_all_, theta_all_, peaks_all_, \n",
    "                                 mean_offset_all_, row_all_), axis=1)\n",
    "\n",
    "## fixes the double labeling of white matter... kinda hacky though (where there were two dict entries for white matter)\n",
    "for index in np.where(target_all_ == '')[0]:\n",
    "    target_all_[index] = 'white matter'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "89d86cba-7fb4-43ae-abdd-290458088c51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3366L, 10L)\n"
     ]
    }
   ],
   "source": [
    "print all_bands_data.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
